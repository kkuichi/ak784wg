{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc38f39-6031-4ae6-a8ac-12f2f82fde57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5876ff9-ead7-4e9e-adf5-c4793d23b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f11859-82c0-42fb-8928-9d9b84f1597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37248a1f-214d-40e8-a585-3779abc87d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4a2d9-4d23-439b-bf42-beca2bbf8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee7c53-4359-428e-93f2-5d237a696c28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f21ba8dd-84fb-47e2-bacb-e0cf3dc494bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import torch\n",
    "from argparse import Namespace\n",
    "from transformers import set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.nn.functional import softmax\n",
    "from matplotlib import pyplot as plt\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"Change runtime type to include a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad35c67-4d54-4979-84db-082382b86628",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99989acd-8ef3-41ff-a42d-b298c6d2aada",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## --Probe helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7aa472-80e9-494d-b18c-7c61f67528ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probe_function(prefix):\n",
    "    probe_functions = [\n",
    "        probe_gpt,\n",
    "        probe_bert,\n",
    "        probe_llama,\n",
    "        probe_t5,\n",
    "        probe_stablelm,\n",
    "        probe_mpt,\n",
    "        probe_redpajama,\n",
    "        probe_falcon,\n",
    "    ]\n",
    "    for func in probe_functions:\n",
    "        if prefix.lower() in func.__name__:\n",
    "            return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161326e9-0e27-48f4-97ea-85eb2b0f78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_t5(model, tokenizer, target_id, context):\n",
    "    # tokenize context\n",
    "    input_ids = tokenizer(\n",
    "        context,\n",
    "        padding=\"longest\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "    # use model to solicit a prediction\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        output_scores=True,\n",
    "        return_dict=True,\n",
    "        return_dict_in_generate=True,\n",
    "        max_new_tokens=4,\n",
    "    )\n",
    "\n",
    "    # find the left-most non-sepecial token, save itr of this token to grab\n",
    "    # correct logit scores array\n",
    "    sequences = outputs[\"sequences\"][0].tolist()\n",
    "    for i in range(4):\n",
    "        logits = outputs[\"scores\"][i]\n",
    "        probs = softmax(logits, dim=-1)\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        if tokenizer.decode([np.argmax(probs)]) not in [\n",
    "            \"<extra_id_0>\",\n",
    "            \"\",\n",
    "            \" \",\n",
    "            \"<pad>\",\n",
    "        ]:\n",
    "            save_itr = i\n",
    "            break\n",
    "    # grab its logits\n",
    "    logits = outputs[\"scores\"][save_itr]\n",
    "    # convert our prediction scores to a probability distribution with softmax\n",
    "    probs = softmax(logits, dim=-1)\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "\n",
    "    return probs[0][target_id.item()]\n",
    "\n",
    "\n",
    "def probe_stablelm(model, tokenizer, target_id, context):\n",
    "    # tokenize context\n",
    "    input_ids = tokenizer(\n",
    "        context,\n",
    "        padding=\"longest\",\n",
    "        max_length=4096,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "    # print(input_ids)\n",
    "    # use model to solicit a prediction\n",
    "    outputs = model.generate(\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        input_ids=input_ids.to(device),\n",
    "        output_scores=True,\n",
    "        return_dict=True,\n",
    "        return_dict_in_generate=True,\n",
    "        max_new_tokens=4,\n",
    "    )\n",
    "    # print(outputs)\n",
    "\n",
    "    # find the left-most non-sepecial token, save itr of this token to grab\n",
    "    # correct logit scores array\n",
    "    sequences = outputs[\"sequences\"][0].tolist()\n",
    "    for i in range(4):\n",
    "        logits = outputs[\"scores\"][i]\n",
    "        probs = softmax(logits, dim=-1)\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        if tokenizer.decode([np.argmax(probs)]) not in [\n",
    "            \"<|endoftext|>\",\n",
    "            \"<|padding|>\",\n",
    "            \"\",\n",
    "            \" \",\n",
    "        ]:\n",
    "            save_itr = i\n",
    "            break\n",
    "    # grab its logits\n",
    "    logits = outputs[\"scores\"][save_itr]\n",
    "    # convert our prediction scores to a probability distribution with softmax\n",
    "    probs = softmax(logits, dim=-1)\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "\n",
    "    return probs[0][target_id.item()]\n",
    "\n",
    "\n",
    "def probe_falcon(model, tokenizer, target_id, context):\n",
    "    # tokenize context\n",
    "    input_ids = tokenizer(\n",
    "        context,\n",
    "        padding=\"longest\",\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "    # use model to solicit a prediction\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        output_scores=True,\n",
    "        return_dict=True,\n",
    "        return_dict_in_generate=True,\n",
    "        max_new_tokens=3,\n",
    "    )\n",
    "\n",
    "    # find the left-most non-sepecial token, save itr of this token to grab\n",
    "    # correct logit scores array\n",
    "    sequences = outputs[\"sequences\"][0].tolist()\n",
    "    for i in range(3):\n",
    "        logits = outputs[\"scores\"][i]\n",
    "        probs = softmax(logits, dim=-1)\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        if tokenizer.decode([np.argmax(probs)]) not in [\n",
    "            \"<|endoftext|>\",\n",
    "            \"<|padding|>\",\n",
    "            \"\",\n",
    "            \" \",\n",
    "        ]:\n",
    "            save_itr = i\n",
    "            break\n",
    "    # grab its logits\n",
    "    logits = outputs[\"scores\"][save_itr]\n",
    "    # convert our prediction scores to a probability distribution with softmax\n",
    "    probs = softmax(logits, dim=-1)\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "\n",
    "    return probs[0][target_id.item()]\n",
    "\n",
    "\n",
    "def probe_redpajama(model, tokenizer, target_id, context):\n",
    "    # tokenize context\n",
    "    input_ids = tokenizer(\n",
    "        context,\n",
    "        padding=\"longest\",\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "    # use model to solicit a prediction\n",
    "    outputs = model.generate(\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        input_ids=input_ids.to(device),\n",
    "        output_scores=True,\n",
    "        return_dict=True,\n",
    "        return_dict_in_generate=True,\n",
    "        max_new_tokens=4,\n",
    "    )\n",
    "\n",
    "    # find the left-most non-sepecial token, save itr of this token to grab\n",
    "    # correct logit scores array\n",
    "    sequences = outputs[\"sequences\"][0].tolist()\n",
    "    for i in range(4):\n",
    "        logits = outputs[\"scores\"][i]\n",
    "        probs = softmax(logits, dim=-1)\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        if tokenizer.decode([np.argmax(probs)]) not in [\n",
    "            \"<|endoftext|>\",\n",
    "            \"<|padding|>\",\n",
    "            \"\",\n",
    "            \" \",\n",
    "        ]:\n",
    "            save_itr = i\n",
    "            break\n",
    "    # grab its logits\n",
    "    logits = outputs[\"scores\"][save_itr]\n",
    "    # convert our prediction scores to a probability distribution with softmax\n",
    "    probs = softmax(logits, dim=-1)\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "\n",
    "    return probs[0][target_id.item()]\n",
    "\n",
    "\n",
    "def probe_mpt(model, tokenizer, target_id, context):\n",
    "    # tokenize context\n",
    "    input_ids = tokenizer(\n",
    "        context,\n",
    "        padding=\"longest\",\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "    # use model to solicit a prediction\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        output_scores=True,\n",
    "        return_dict=True,\n",
    "        return_dict_in_generate=True,\n",
    "        max_new_tokens=4,\n",
    "    )\n",
    "\n",
    "    # find the left-most non-sepecial token, save itr of this token to grab\n",
    "    # correct logit scores array\n",
    "    sequences = outputs[\"sequences\"][0].tolist()\n",
    "    for i in range(4):\n",
    "        logits = outputs[\"scores\"][i]\n",
    "        probs = softmax(logits, dim=-1)\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        if tokenizer.decode([np.argmax(probs)]) not in [\n",
    "            \"<|endoftext|>\",\n",
    "            \"<|padding|>\",\n",
    "            \"\",\n",
    "            \" \",\n",
    "        ]:\n",
    "            save_itr = i\n",
    "            break\n",
    "    # grab its logits\n",
    "    logits = outputs[\"scores\"][save_itr]\n",
    "    # convert our prediction scores to a probability distribution with softmax\n",
    "    probs = softmax(logits, dim=-1)\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "\n",
    "    return probs[0][target_id.item()]\n",
    "\n",
    "\n",
    "def probe_gpt(model, tokenizer, target_id, context):\n",
    "    # tokenize context\n",
    "    input_ids = tokenizer(\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids.to(device)\n",
    "\n",
    "    # grab value\n",
    "    target_scalar = target_id.detach().cpu().numpy()\n",
    "\n",
    "    # use model to solicit a prediction\n",
    "    outputs = model(input_ids=input_ids, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    # every token in the model's vocab gets a representative prediction from the model\n",
    "    logits = outputs[\"logits\"][0, -1]\n",
    "    # convert our prediction scores to a probability distribution with softmax\n",
    "    probs = softmax(logits, dim=-1)\n",
    "    probs = list(probs.detach().cpu().numpy())\n",
    "\n",
    "    # double check weird-ness before accessing prob\n",
    "    if len(probs) < target_id:\n",
    "        return None\n",
    "\n",
    "    # return the likelihood that our stipulated target would follow the context,\n",
    "    # according to the model\n",
    "    try:\n",
    "        return np.take(probs, [target_scalar])[0]\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"target index not in model vocabulary scope; raising IndexError\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def probe_bert(model, tokenizer, target_id, context):\n",
    "    # tokenize context\n",
    "    input_ids = tokenizer(\n",
    "        context,\n",
    "        padding=\"longest\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # use model to solicit a prediction\n",
    "    logits = model(input_ids=input_ids.to(device)).logits\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "    # Convert our prediction scores to a probability distribution with softmax\n",
    "    probs = torch.squeeze(softmax(mask_token_logits, dim=-1))\n",
    "\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "\n",
    "    return probs[target_id.item()]\n",
    "\n",
    "\n",
    "def probe_llama(model, tokenizer, target_id, context):\n",
    "    # tokenize context\n",
    "    input_ids = tokenizer(\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids.to(device)\n",
    "\n",
    "    # grab value\n",
    "    target_scalar = target_id.detach().cpu().numpy()\n",
    "\n",
    "    # use model to solicit a prediction\n",
    "    outputs = model(input_ids=input_ids, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    # every token in the model's vocab gets a representative prediction from the model\n",
    "    logits = outputs[\"logits\"][0, -1]\n",
    "    # convert our prediction scores to a probability distribution with softmax\n",
    "    probs = softmax(logits, dim=-1)\n",
    "\n",
    "    probs = list(probs.detach().cpu().numpy())\n",
    "\n",
    "    # double check weird-ness before accessing prob\n",
    "    if len(probs) < target_id:\n",
    "        return None\n",
    "\n",
    "    # return the likelihood that our stipulated target would follow the context,\n",
    "    # according to the model\n",
    "    try:\n",
    "        return np.take(probs, [target_scalar])[0]\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"target index not in model vocabulary scope; raising IndexError\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834be1ff-db36-425d-8011-facb2b04d2a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## --Get model token function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c3d04c-18ab-45bc-a962-5468c9cb142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_name):\n",
    "    if \"t5\" in model_name.lower():\n",
    "        return AutoTokenizer.from_pretrained(\n",
    "            model_name\n",
    "        ), AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name, \n",
    "            # load_in_8bit=True, \n",
    "            # quantization_config = quantization_config,\n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    elif (\n",
    "        (\"gpt\" in model_name.lower())\n",
    "        or (\"opt\" in model_name.lower())\n",
    "        or (\"pythia\" in model_name.lower())\n",
    "        or (\"bloom\" in model_name.lower())\n",
    "    ):\n",
    "        return AutoTokenizer.from_pretrained(\n",
    "            model_name\n",
    "        ), AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            load_in_8bit=True, \n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    elif (\"stablelm\" in model_name.lower()) or (\"redpajama\" in model_name.lower()):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = \"<|padding|>\"\n",
    "        \n",
    "        # bnb_config = transformers.BitsAndBytesConfig(\n",
    "        #     load_in_8bit=True,\n",
    "        #     # bnb_4bit_use_double_quant=True,\n",
    "        #     # bnb_4bit_quant_type=\"nf4\",\n",
    "        #     bnb_8bit_compute_dtype=torch.float16,\n",
    "        # )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            # load_in_8bit=True, \n",
    "            # quantization_config=bnb_config,\n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.float16,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            offload_folder='./offload'\n",
    "        )\n",
    "        return tokenizer, model\n",
    "\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = \"<|padding|>\"\n",
    "\n",
    "        bnb_config = transformers.BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map={\"\": 0},\n",
    "            # torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "        )  # .to(device)\n",
    "\n",
    "        return (\n",
    "            tokenizer,\n",
    "            model,\n",
    "        )\n",
    "\n",
    "    elif \"bert\" in model_name.lower():\n",
    "        return AutoTokenizer.from_pretrained(\n",
    "            model_name\n",
    "        ), AutoModelForMaskedLM.from_pretrained(\n",
    "            model_name, torch_dtype=torch.float16\n",
    "        ).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "    elif \"llama\" in model_name.lower():\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            load_in_8bit=True, \n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        return tokenizer, model\n",
    "\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            # load_in_8bit=True, \n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        return tokenizer, model\n",
    "\n",
    "    elif \"falcon\" in model_name.lower():\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        bnb_config = transformers.BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            # torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "        )  # .to(device)\n",
    "\n",
    "        return (\n",
    "            tokenizer,\n",
    "            model,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22940d2f-2df2-4314-8952-1c60236e8b79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## --Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ba4cf4-1d76-4e61-99d9-dba723dd581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, input_dataset, tokenizer, model, lang, p_true_freq_list):\n",
    "\n",
    "    true_count = 0\n",
    "    fact_count = 0\n",
    "    p_falses = []\n",
    "    p_trues = []\n",
    "\n",
    "    # establish prefix\n",
    "    prefix = \"\"\n",
    "    probe_func = None\n",
    "\n",
    "    # get correct CKA function\n",
    "    if \"t5\" in model_name.lower():\n",
    "        prefix = \"t5\"\n",
    "        probe_func = get_probe_function(prefix)\n",
    "    elif (\n",
    "        (\"gpt-neo\" in model_name.lower())\n",
    "        or (\"gpt-j\" in model_name.lower())\n",
    "        or (\"pythia\" in model_name.lower())\n",
    "    ):\n",
    "        prefix = \"eleutherai\"\n",
    "        probe_func = get_probe_function(\"gpt\")\n",
    "\n",
    "    elif \"gpt\" in model_name.lower():\n",
    "        prefix = \"gpt\"\n",
    "        probe_func = get_probe_function(prefix)\n",
    "\n",
    "    elif \"opt\" in model_name.lower():\n",
    "        prefix = \"opt\"\n",
    "        probe_func = get_probe_function(\"gpt\")\n",
    "\n",
    "    elif \"roberta\" in model_name.lower():\n",
    "        prefix = \"roberta\"\n",
    "        probe_func = get_probe_function(\"bert\")\n",
    "\n",
    "    elif \"bert\" in model_name.lower():\n",
    "        prefix = \"bert\"\n",
    "        probe_func = get_probe_function(prefix)\n",
    "\n",
    "    elif \"llama\" in model_name.lower():\n",
    "        prefix = \"llama\"\n",
    "        probe_func = get_probe_function(prefix)\n",
    "\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        prefix = \"mistral\"\n",
    "        probe_func = get_probe_function(\"llama\")\n",
    "\n",
    "    elif \"bloom\" in model_name.lower():\n",
    "        prefix = \"bloom\"\n",
    "        probe_func = get_probe_function(\"gpt\")\n",
    "\n",
    "    elif \"stablelm\" in model_name.lower():\n",
    "        prefix = \"stablelm\"\n",
    "        probe_func = get_probe_function(prefix)\n",
    "\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        prefix = \"mpt\"\n",
    "        probe_func = get_probe_function(prefix)\n",
    "\n",
    "    elif \"redpajama\" in model_name.lower():\n",
    "        prefix = \"redpajama\"\n",
    "        probe_func = get_probe_function(prefix)\n",
    "\n",
    "    elif \"falcon\" in model_name.lower():\n",
    "        prefix = \"falcon\"\n",
    "        probe_func = get_probe_function(prefix)\n",
    "\n",
    "    # iterate over context/entity pairings\n",
    "    # input_dataset is a datasets dataset\n",
    "    # context is a plain string (since our context's will be unique)\n",
    "    # and entities is a list containing, in the first slot, the true\n",
    "    # value for the statement and in the subsequent slots, incorrect information\n",
    "    \n",
    "    conf_idx_sum = 0\n",
    "    conf_idx_count = 0\n",
    "    for entities_dict in tqdm.tqdm(input_dataset):\n",
    "        # convert string of list into a real list\n",
    "        if \" <br> \" in entities_dict[\"false\"]:\n",
    "            counterfacts_list = entities_dict[\"false\"].split(\" <br> \")\n",
    "        else:\n",
    "            counterfacts_list = [entities_dict[\"false\"]]\n",
    "\n",
    "        # intitiate vars\n",
    "        p_true = 0.0\n",
    "        p_false = 0.0\n",
    "        p_false_list_inner = []\n",
    "\n",
    "        # grab true and false entities\n",
    "        entities = [entities_dict[\"true\"]]\n",
    "        entities.extend(counterfacts_list)\n",
    "\n",
    "        # iterate through each fact and counterfact\n",
    "        for entity_count, entity in enumerate(entities):\n",
    "            # grab the context\n",
    "            context = entities_dict[\"stem\"]\n",
    "            # if multiple stems are stored, grab the correct one\n",
    "            # (zeroeth stem is true fact, next ones are counterfacts)\n",
    "            if \" <br> \" in context:\n",
    "                context = context.split(\" <br> \")\n",
    "            if type(context) == list:\n",
    "                context = context[entity_count]\n",
    "            # necessary additions based on model type\n",
    "            if prefix == \"roberta\":\n",
    "                context += \" <mask>.\"\n",
    "            elif prefix == \"bert\":\n",
    "                context += \" [MASK].\"\n",
    "\n",
    "            # first find target vocab id\n",
    "            # default to the very first token that get's predicted\n",
    "            # e.g. in the case of Tokyo, which gets split into <Tok> <yo>,\n",
    "            target_id = None\n",
    "            if prefix == \"t5\":\n",
    "                target_ids = tokenizer.encode(\n",
    "                    \" \" + entity,\n",
    "                    padding=\"longest\",\n",
    "                    max_length=512,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).tolist()\n",
    "                space_only_token = tokenizer.encode(\" \")[0]\n",
    "                try:\n",
    "                    target_ids[0].remove(space_only_token)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                target_id = torch.tensor(target_ids).to(device)[0][0]\n",
    "\n",
    "            elif (\n",
    "                (prefix == \"gpt\")\n",
    "                or (prefix == \"eleutherai\")\n",
    "                or (prefix == \"bloom\")\n",
    "                or (prefix == \"stablelm\")\n",
    "                or (prefix == \"mpt\")\n",
    "                or (prefix == \"redpajama\")\n",
    "            ):\n",
    "                target_id = tokenizer.encode(\" \" + entity, return_tensors=\"pt\").to(\n",
    "                    device\n",
    "                )[0][0]\n",
    "\n",
    "            elif prefix == \"falcon\":\n",
    "                target_id = tokenizer.encode(\n",
    "                    \" \" + entity, return_token_type_ids=False, return_tensors=\"pt\"\n",
    "                ).to(device)[0][0]\n",
    "\n",
    "            elif prefix == \"opt\":\n",
    "                target_id = tokenizer.encode(\" \" + entity, return_tensors=\"pt\").to(\n",
    "                    device\n",
    "                )[0][1]\n",
    "\n",
    "            elif prefix == \"roberta\":\n",
    "                target_id = tokenizer.encode(\n",
    "                    \" \" + entity,\n",
    "                    padding=\"longest\",\n",
    "                    max_length=512,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(device)[0][1]\n",
    "\n",
    "            elif prefix == \"bert\":\n",
    "                target_id = tokenizer.encode(\n",
    "                    entity,\n",
    "                    padding=\"longest\",\n",
    "                    max_length=512,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(device)[0][1]\n",
    "\n",
    "            elif (prefix == \"llama\") or (prefix == \"mistral\"):\n",
    "                target_id = tokenizer.encode(\" \" + entity, return_tensors=\"pt\").to(\n",
    "                    device\n",
    "                )[0][2]\n",
    "\n",
    "            # next call probe function\n",
    "            model_prob = probe_func(model, tokenizer, target_id, context)\n",
    "\n",
    "            # lastly, register results\n",
    "            # if it is the first time through, it is the fact\n",
    "            if entity_count == 0:\n",
    "                p_true = model_prob\n",
    "            # if it is the second+ time through, it is the counterfactual(s)\n",
    "            else:\n",
    "                p_false += model_prob\n",
    "                p_false_list_inner.append(float(model_prob))\n",
    "        \n",
    "        # logging data for position analytics\n",
    "        p_false_list_inner.sort(reverse=True)\n",
    "        pos=1\n",
    "        for i in range(len(p_false_list_inner)):\n",
    "            if(p_true > p_false_list_inner[i]):\n",
    "                break\n",
    "            pos+=1\n",
    "        p_true_freq_list[0].append(pos)\n",
    "        p_true_freq_list[1].append(lang)\n",
    "\n",
    "        \n",
    "        p_false_list_inner.append(p_true)\n",
    "        p_false_list_inner.sort(reverse=True)\n",
    "        \n",
    "        if not (p_false_list_inner[0] == 0 or p_false_list_inner[1] == 0):\n",
    "            conf_idx_sum += p_false_list_inner[0]/p_false_list_inner[1]\n",
    "            conf_idx_count+=1\n",
    "            \n",
    "        \n",
    "        # entity count is equal to the num counterfactuals\n",
    "        # (since it started at a 0 index in the enumerate)\n",
    "        p_false /= entity_count\n",
    "\n",
    "        # append p_false and p_true\n",
    "        p_falses.append(float(p_false))\n",
    "        p_trues.append(float(p_true))\n",
    "\n",
    "        # update counts based on probs\n",
    "        if p_true > p_false:\n",
    "            true_count += 1\n",
    "        fact_count += 1\n",
    "    \n",
    "    return true_count/fact_count, round(conf_idx_sum/conf_idx_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048aafc-cc3c-4c66-9e19-2183cccd565d",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0cacb-509b-48e1-a96b-335dde585466",
   "metadata": {},
   "source": [
    "## --Benchmark arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db747d0-4e62-4724-925e-e2c42262acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark arguments\n",
    "\n",
    "model_names = [\n",
    "    'bert-base-multilingual-cased',\n",
    "    'xlm-roberta-large',\n",
    "    'xlm-roberta-base',\n",
    "    'google/mt5-small',\n",
    "    'google/mt5-large',\n",
    "    'stabilityai/stablelm-3b-4e1t',\n",
    "    'stabilityai/stablelm-zephyr-3b',\n",
    "    'togethercomputer/RedPajama-INCITE-Base-3B-v1'\n",
    "]\n",
    "\n",
    "langs = ['English', 'Czech', 'Ukrainian']\n",
    "\n",
    "data_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e86064-7827-4dca-9789-6baabecb9edc",
   "metadata": {},
   "source": [
    "## -- Checking arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beab19ba-0ac0-44c0-8e7c-23b365ad3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model names\n",
    "compatible_model_prefixes = [\n",
    "    \"t5\",\n",
    "    \"pythia\",\n",
    "    \"gpt\",\n",
    "    \"opt\",\n",
    "    \"llama\",\n",
    "    \"roberta\",\n",
    "    \"bert\",\n",
    "    \"bloom\",\n",
    "    \"stablelm\",\n",
    "    \"mpt\",\n",
    "    \"redpajama\",\n",
    "    \"falcon\",\n",
    "    \"mistral\",\n",
    "]\n",
    "for model_name in model_names:\n",
    "    model_supported = True\n",
    "    for model_prefix in compatible_model_prefixes:\n",
    "        if model_prefix in model_name.lower():\n",
    "            model_supported = True\n",
    "            break\n",
    "    if not model_supported:\n",
    "        raise Exception(f\"Model {model_name} not supported.\")\n",
    "\n",
    "\n",
    "# Check language name\n",
    "supported_languages = [\n",
    "    \"English\", \n",
    "    \"French\", \n",
    "    \"Spanish\", \n",
    "    \"German\", \n",
    "    \"Ukrainian\", \n",
    "    \"Romanian\", \n",
    "    \"Bulgarian\", \n",
    "    \"Catalan\", \n",
    "    \"Danish\", \n",
    "    \"Croatian\", \n",
    "    \"Hungarian\", \n",
    "    \"Italian\", \n",
    "    \"Dutch\", \n",
    "    \"Polish\", \n",
    "    \"Portuguese\", \n",
    "    \"Russian\", \n",
    "    \"Slovenian\", \n",
    "    \"Serbian\", \n",
    "    \"Swedish\", \n",
    "    \"Czech\"\n",
    "]\n",
    "for lang in langs:\n",
    "    if lang not in supported_languages:\n",
    "        raise Exception(f'Language {lang} not supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc15081-c8d9-4b4c-8186-cb88ad1ccfb5",
   "metadata": {},
   "source": [
    "## -- Dataset download/read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5521079-c332-4476-89be-ca60c4fd2ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for english language loaded from local storage successfully!\n",
      "Dataset for czech language loaded from local storage successfully!\n",
      "Dataset for ukrainian language loaded from local storage successfully!\n"
     ]
    }
   ],
   "source": [
    "# Datasets downloading or reading\n",
    "datasets = []\n",
    "\n",
    "local_dataset_path = './local_datasets'\n",
    "\n",
    "if not os.path.exists(local_dataset_path):\n",
    "    os.makedirs(local_dataset_path)\n",
    "\n",
    "for lang in langs:\n",
    "    local_path = os.path.join(local_dataset_path, f'{lang}_dataset')\n",
    "    if os.path.exists(local_path):\n",
    "        dataset = load_from_disk(local_path)\n",
    "        print(f'Dataset for {lang.lower()} language loaded from local storage successfully!')\n",
    "    else:\n",
    "        dataset = load_dataset(\"Polyglot-or-Not/Fact-Completion\", split=lang).select(range(data_size))\n",
    "        dataset.save_to_disk(local_path)\n",
    "        print(f'Dataset for {lang.lower()} language downloaded and saved locally successfully!')\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb510a10-e6d5-4fb2-8239-bd42a52ce829",
   "metadata": {},
   "source": [
    "## --Create/read tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f7d008-ab82-443a-a533-60773c9cfa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df_name = 'accuracy_table.csv'\n",
    "fact_pos_name = 'fact_position_stats.csv'\n",
    "conf_idx_name = 'confidence_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a927cac-6c76-41aa-a7f6-f59b98b59b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>English</th>\n",
       "      <th>Czech</th>\n",
       "      <th>Ukrainian</th>\n",
       "      <th>Model type</th>\n",
       "      <th>Params count (mil)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>15911</td>\n",
       "      <td>2509</td>\n",
       "      <td>1440</td>\n",
       "      <td>MaskedLM</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xlm-roberta-large</td>\n",
       "      <td>35580</td>\n",
       "      <td>58335</td>\n",
       "      <td>52950</td>\n",
       "      <td>MaskedLM</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>20401</td>\n",
       "      <td>11904</td>\n",
       "      <td>25385</td>\n",
       "      <td>MaskedLM</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mt5-small</td>\n",
       "      <td>1740</td>\n",
       "      <td>5140</td>\n",
       "      <td>3622</td>\n",
       "      <td>text2text</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mt5-large</td>\n",
       "      <td>1997</td>\n",
       "      <td>10427</td>\n",
       "      <td>9913</td>\n",
       "      <td>text2text</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stablelm-3b-4e1t</td>\n",
       "      <td>56429</td>\n",
       "      <td>77932</td>\n",
       "      <td>6022</td>\n",
       "      <td>CausalLM</td>\n",
       "      <td>2795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stablelm-zephyr-3b</td>\n",
       "      <td>170648</td>\n",
       "      <td>36613</td>\n",
       "      <td>2108</td>\n",
       "      <td>CausalLM</td>\n",
       "      <td>2795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RedPajama-INCITE-Base-3B-v1</td>\n",
       "      <td>40689</td>\n",
       "      <td>53117</td>\n",
       "      <td>5287</td>\n",
       "      <td>CausalLM</td>\n",
       "      <td>2776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model name  English  Czech  Ukrainian Model type  \\\n",
       "0  bert-base-multilingual-cased    15911   2509       1440   MaskedLM   \n",
       "1             xlm-roberta-large    35580  58335      52950   MaskedLM   \n",
       "2              xlm-roberta-base    20401  11904      25385   MaskedLM   \n",
       "3                     mt5-small     1740   5140       3622  text2text   \n",
       "4                     mt5-large     1997  10427       9913  text2text   \n",
       "5              stablelm-3b-4e1t    56429  77932       6022   CausalLM   \n",
       "6            stablelm-zephyr-3b   170648  36613       2108   CausalLM   \n",
       "7   RedPajama-INCITE-Base-3B-v1    40689  53117       5287   CausalLM   \n",
       "\n",
       "   Params count (mil)  \n",
       "0                 178  \n",
       "1                 560  \n",
       "2                 278  \n",
       "3                 300  \n",
       "4                1230  \n",
       "5                2795  \n",
       "6                2795  \n",
       "7                2776  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conf_idx_df = pd.DataFrame(columns = ['Model name'] + langs)\n",
    "conf_idx_df = pd.read_csv(conf_idx_name)\n",
    "conf_idx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb992f6-48e7-4282-95f2-d6baa1120866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>English</th>\n",
       "      <th>Czech</th>\n",
       "      <th>Ukrainian</th>\n",
       "      <th>Param count (mil)</th>\n",
       "      <th>Mean accuracy</th>\n",
       "      <th>Model type</th>\n",
       "      <th>Efficienty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.562</td>\n",
       "      <td>178</td>\n",
       "      <td>0.636</td>\n",
       "      <td>MaskedLM</td>\n",
       "      <td>35.730337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xlm-roberta-large</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.554</td>\n",
       "      <td>560</td>\n",
       "      <td>0.575</td>\n",
       "      <td>MaskedLM</td>\n",
       "      <td>10.267857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.536</td>\n",
       "      <td>278</td>\n",
       "      <td>0.572</td>\n",
       "      <td>MaskedLM</td>\n",
       "      <td>20.575540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mt5-small</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.475</td>\n",
       "      <td>300</td>\n",
       "      <td>0.484</td>\n",
       "      <td>text2text</td>\n",
       "      <td>16.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mt5-large</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1230</td>\n",
       "      <td>0.520</td>\n",
       "      <td>text2text</td>\n",
       "      <td>4.227642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stablelm-3b-4e1t</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.632</td>\n",
       "      <td>2795</td>\n",
       "      <td>0.731</td>\n",
       "      <td>CausalLM</td>\n",
       "      <td>2.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stablelm-zephyr-3b</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.521</td>\n",
       "      <td>2795</td>\n",
       "      <td>0.612</td>\n",
       "      <td>CausalLM</td>\n",
       "      <td>2.189624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RedPajama-INCITE-Base-3B-v1</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.605</td>\n",
       "      <td>2776</td>\n",
       "      <td>0.704</td>\n",
       "      <td>CausalLM</td>\n",
       "      <td>2.536023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model name  English  Czech  Ukrainian  Param count (mil)  \\\n",
       "0  bert-base-multilingual-cased    0.714  0.632      0.562                178   \n",
       "1             xlm-roberta-large    0.599  0.573      0.554                560   \n",
       "2              xlm-roberta-base    0.610  0.570      0.536                278   \n",
       "3                     mt5-small    0.517  0.461      0.475                300   \n",
       "4                     mt5-large    0.575  0.484      0.502               1230   \n",
       "5              stablelm-3b-4e1t    0.848  0.713      0.632               2795   \n",
       "6            stablelm-zephyr-3b    0.743  0.571      0.521               2795   \n",
       "7   RedPajama-INCITE-Base-3B-v1    0.822  0.684      0.605               2776   \n",
       "\n",
       "   Mean accuracy Model type  Efficienty  \n",
       "0          0.636   MaskedLM   35.730337  \n",
       "1          0.575   MaskedLM   10.267857  \n",
       "2          0.572   MaskedLM   20.575540  \n",
       "3          0.484  text2text   16.133333  \n",
       "4          0.520  text2text    4.227642  \n",
       "5          0.731   CausalLM    2.615385  \n",
       "6          0.612   CausalLM    2.189624  \n",
       "7          0.704   CausalLM    2.536023  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acc_df = pd.DataFrame(columns = ['Model name'] + langs + ['Param count (mil)'])\n",
    "acc_df = pd.read_csv(acc_df_name)\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc7b2e-ac5e-4498-8c34-8dc40831ee42",
   "metadata": {},
   "source": [
    "## --Benchmark itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b778643-65c4-4b9c-b2a5-b9ae11f6d8c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m stat_pos_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_names\u001b[49m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m acc_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist():\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_names' is not defined"
     ]
    }
   ],
   "source": [
    "stat_pos_dict = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    if model_name.split('/')[-1] in acc_df['Model name'].tolist():\n",
    "        continue\n",
    "    \n",
    "    stat_pos = [[], []]\n",
    "    \n",
    "    tokenizer, model = get_model_and_tokenizer(model_name)\n",
    "    model_name = model_name.split('/')[-1]\n",
    "    print(f'\\n\\n{model_name}')\n",
    "    \n",
    "    \n",
    "    acc_row = pd.DataFrame(columns = ['Model name']+langs+['Param count (mil)'], data={'Model name': model_name}, index=[0])\n",
    "    acc_row.loc[0, 'Model name'] = model_name\n",
    "    acc_row.loc[0, 'Param count (mil)'] = round(sum(p.numel() for p in model.parameters())/1000000)\n",
    "    # print(acc_row)\n",
    "\n",
    "    conf_row = pd.DataFrame(columns = ['Model name'] + langs, data={'Model name': model_name}, index=[0])\n",
    "    conf_row.loc[0, 'Model name'] = model_name\n",
    "    # print(conf_row)\n",
    "    \n",
    "    for j in range(1, len(langs)+1):\n",
    "        acc_row.iloc[0, j],  conf_row.iloc[0, j]= evaluate_model(model_name, datasets[j-1], tokenizer, model, langs[j-1], stat_pos)\n",
    "        print(f\"{langs[j-1]} language: accuracy is {acc_row.iloc[0, j]}, confidence index is {conf_row.iloc[0, j]}\")\n",
    "    \n",
    "    # print(acc_row)\n",
    "    \n",
    "    # logging data for accuracy and confidence\n",
    "    acc_df = pd.concat([acc_df, acc_row], ignore_index=True)\n",
    "    conf_idx_df = pd.concat([conf_idx_df, conf_row], ignore_index=True)\n",
    "    \n",
    "    if 'Language' not in stat_pos_dict:\n",
    "        stat_pos_dict['Language'] = stat_pos[1]\n",
    "    stat_pos_dict[model_name] = stat_pos[0]\n",
    "\n",
    "    gc.collect()\n",
    "    del tokenizer\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585503f0-0212-4eb3-90e3-ac0fcd233444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of a loop crash, clear memory\n",
    "\n",
    "# gc.collect()\n",
    "# del tokenizer\n",
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64dced-0807-419b-9bdd-e62dad9a32df",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1d6209f-9870-4600-8900-b41c1a1a4569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>stablelm-3b-4e1t</th>\n",
       "      <th>stablelm-zephyr-3b</th>\n",
       "      <th>RedPajama-INCITE-Base-3B-v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language  stablelm-3b-4e1t  stablelm-zephyr-3b  RedPajama-INCITE-Base-3B-v1\n",
       "0  English                 1                   2                            1\n",
       "1  English                 1                   1                            1\n",
       "2  English                 1                   1                            1\n",
       "3  English                 1                   1                            1\n",
       "4  English                 1                   1                            1\n",
       "5  English                 1                   1                            1\n",
       "6  English                 1                   1                            1\n",
       "7  English                 1                   1                            1\n",
       "8  English                 1                   1                            1\n",
       "9  English                 1                   2                            1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_pos_df = pd.DataFrame(stat_pos_dict)\n",
    "fact_pos_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a042dd12-7b73-4b50-9035-64644875e320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>English</th>\n",
       "      <th>Czech</th>\n",
       "      <th>Ukrainian</th>\n",
       "      <th>Param count (mil)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.562</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xlm-roberta-large</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.554</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.536</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mt5-small</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.475</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mt5-large</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stablelm-3b-4e1t</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.632</td>\n",
       "      <td>2795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stablelm-zephyr-3b</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.521</td>\n",
       "      <td>2795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RedPajama-INCITE-Base-3B-v1</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.605</td>\n",
       "      <td>2776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model name English  Czech Ukrainian Param count (mil)\n",
       "0  bert-base-multilingual-cased   0.714  0.632     0.562               178\n",
       "1             xlm-roberta-large   0.599  0.573     0.554               560\n",
       "2              xlm-roberta-base    0.61   0.57     0.536               278\n",
       "3                     mt5-small   0.517  0.461     0.475               300\n",
       "4                     mt5-large   0.575  0.484     0.502              1230\n",
       "5              stablelm-3b-4e1t   0.848  0.713     0.632              2795\n",
       "6            stablelm-zephyr-3b   0.743  0.571     0.521              2795\n",
       "7   RedPajama-INCITE-Base-3B-v1   0.822  0.684     0.605              2776"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c8092ac-f46d-4c7b-b2d8-7be1df01d3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>English</th>\n",
       "      <th>Czech</th>\n",
       "      <th>Ukrainian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>15911</td>\n",
       "      <td>2509</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xlm-roberta-large</td>\n",
       "      <td>35580</td>\n",
       "      <td>58335</td>\n",
       "      <td>52950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>20401</td>\n",
       "      <td>11904</td>\n",
       "      <td>25385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mt5-small</td>\n",
       "      <td>1740</td>\n",
       "      <td>5140</td>\n",
       "      <td>3622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mt5-large</td>\n",
       "      <td>1997</td>\n",
       "      <td>10427</td>\n",
       "      <td>9913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stablelm-3b-4e1t</td>\n",
       "      <td>56429</td>\n",
       "      <td>77932</td>\n",
       "      <td>6022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stablelm-zephyr-3b</td>\n",
       "      <td>170648</td>\n",
       "      <td>36613</td>\n",
       "      <td>2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RedPajama-INCITE-Base-3B-v1</td>\n",
       "      <td>40689</td>\n",
       "      <td>53117</td>\n",
       "      <td>5287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model name English  Czech Ukrainian\n",
       "0  bert-base-multilingual-cased   15911   2509      1440\n",
       "1             xlm-roberta-large   35580  58335     52950\n",
       "2              xlm-roberta-base   20401  11904     25385\n",
       "3                     mt5-small    1740   5140      3622\n",
       "4                     mt5-large    1997  10427      9913\n",
       "5              stablelm-3b-4e1t   56429  77932      6022\n",
       "6            stablelm-zephyr-3b  170648  36613      2108\n",
       "7   RedPajama-INCITE-Base-3B-v1   40689  53117      5287"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_idx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a71ed-cb58-4aaa-a345-46186c00b867",
   "metadata": {},
   "source": [
    "## --Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79249f38-05be-49f1-8230-621f588ed806",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.to_csv(acc_df_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8b4249d-3640-41d5-aa1f-39f7c8b95c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_idx_df.to_csv(conf_idx_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db758c51-c6aa-4f25-ab68-605f85fb45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_pos_df.to_csv(fact_pos_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8442158-49c6-4f56-bf26-a1e1bffa5fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
